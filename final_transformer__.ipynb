{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3470456,"sourceType":"datasetVersion","datasetId":2089255},{"sourceId":14227517,"sourceType":"datasetVersion","datasetId":9075668},{"sourceId":14243099,"sourceType":"datasetVersion","datasetId":9087037},{"sourceId":687860,"sourceType":"modelInstanceVersion","modelInstanceId":521643,"modelId":535805},{"sourceId":694465,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":526638,"modelId":540682}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport numpy as np\nimport html\nimport sentencepiece as spm\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom torch.optim.lr_scheduler import LambdaLR\n\n# import spacy\nfrom typing import List\n!pip install sacrebleu\nfrom tqdm import tqdm\nimport sacrebleu\n","metadata":{"id":"rLqUt6MOliqk","executionInfo":{"status":"ok","timestamp":1765818974750,"user_tz":-420,"elapsed":1354,"user":{"displayName":"23020079 Bùi An Huy","userId":"04676746815628624144"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Pre-processing\nThe data is kinda clean tho NO IT IS NOT\n#TODO \n - [x] số liệu thống kê về dữ liệu và từ điển: số câu, vocab size (thật), độ dài câu (min/max/mean), số token?\n - [x] xử lý dữ liệu\n - [x] train tokenizer với vocab size lớn hơn / ko vocab size. tokenizer ko để coverage = 1 nên lỗi\n - [ ] cache tokenization để train ngay\n - [ ] length-based batching để giảm padding\n - [x] real validation data if tst2013 is too small\n - [x] câu ngắn nhất là '\\ '\n - [x] bỏ các câu quá dài (có cần k?) (ko)","metadata":{}},{"cell_type":"code","source":"D_MODEL = 128\nVOCAB_SIZE = 20000\nD_FF = 1024\nLAYERS=6\nH=4\nWARMUP = 4000\n\nBATCH_SIZE = 16\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# just sooooooooo many custom rules\n# mostly deal with html tag remains. someone cleaned the data but kinda messed it up?\ndef clean_sentence(text):\n    currtext = \"\"\n    pattern = re.compile(r'&(?:\\s*amp\\s*;)+', re.IGNORECASE) # a lot of amp\n    while (currtext != text):\n        currtext = text\n        text = re.sub(r'&amp;\\s*lt\\s*;\\s*em\\s*&amp;\\s*gt\\s*;\\s*', r'<', text) # change &amp &amp lt to &lt to unescape later \n        text = re.sub(r'&amp;\\s*lt\\s*;\\s*/\\s*em\\s*&amp;\\s*gt\\s*;\\s*', r'>', text) # same but for &gt\n        text = re.sub(r'&\\s+', '&', text) # cut space between & and whatever (not ideal but idc)\n        # text = pattern.sub(r'\\'', text) \n        # text = re.sub(r'&apos;', r\"'\",text)\n        # text = html.unescape(text) # why is this here? at this point i'm too scared to change it\n        text = re.sub(r'\\s+>', '>', text)\n        text = re.sub(r'>(?=\\w)', '> ', text)         # add space after > if followed by a letter\n        #drop specific stuffs\n        text = re.sub(r'<>','', text)\n        text = re.sub(r'< ; i> ;', '', text) \n        text = re.sub(r' < ; / i> ', '', text)\n        text = re.sub(r'< ; / I> ;', '', text)\n        text = re.sub(r'<\\s*;?\\s*a\\s+href = [^>]*>', '', text, flags=re.IGNORECASE)\n        \n        text = re.sub(r'; (.*?) < ; / a> ;', r'\\1', text) # drop the </a> tags\n        text = re.sub(r'<\\s*(.*?)\\s*>', r'\\1', text) # change <> to \"\"\n        text = re.sub(r'\"([^\"]*?)\\s*([,.])\\s*\"',r'\"\\1 \" \\2',text)\n        text = html.unescape(text)\n    text = re.sub(r\"\\b(\\w+)\\s+'(\\w+)\\b\", r\"\\1'\\2\", text)\n    return text\n\ntext = \" and I don &apos;t care what the politicians spout .\"\nclean_sentence(text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# file_en_path = \"/kaggle/input/iwslt15-englishvietnamese/IWSLT'15 en-vi/train.en.txt\"\n# file_vi_path = \"/kaggle/input/iwslt15-englishvietnamese/IWSLT'15 en-vi/train.vi.txt\"\n\ndef load_file(path):\n    with open(path, encoding=\"utf-8\") as f:\n        lines = [line.rstrip(\"\\n\") for line in f]\n        lines = [clean_sentence(line) for line in lines]\n    return lines\n\n# en = load_file(file_en_path)\n# vi = load_file(file_vi_path)\n\n# print(en[:5])\n# print(vi[:5])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"cleaned_train.vi.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.writelines(line + '\\n' for line in vi)\nwith open(\"cleaned_train.en.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.writelines(line + '\\n' for line in en)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train a tokenizer using sentencepiece lib\n# isn't this a bit too easy???\n# don't run this cell if tokenizer trained\n\nimport sentencepiece as spm\nimport os\n\n# clean_en_path = '/kaggle/input/iwslt15-clean-data/cleaned_train.en.txt'\n# clean_vi_path = '/kaggle/input/iwslt15-clean-data/cleaned_train.vn.txt'\n\n#temporary run\nclean_en_path = '/kaggle/working/cleaned_train.en.txt'\nclean_vi_path = '/kaggle/working/cleaned_train.vi.txt'\nif os.path.getsize(clean_en_path) == 0 or os.path.getsize(clean_vi_path) == 0:\n    raise ValueError(\"input not found???\")\n\ninput_arg = f\"{clean_en_path},{clean_vi_path}\"\n\nprint(f\"Đang train SentencePiece trên: {input_arg}\")\n\nspm.SentencePieceTrainer.train(\n    input=input_arg,            \n    model_prefix=\"spm_en_vi\",\n    vocab_size=VOCAB_SIZE,\n    model_type=\"bpe\",\n    \n    character_coverage=1,   \n    shuffle_input_sentence=True,\n    max_sentence_length=2048,\n    num_threads=2,         \n    \n    pad_id=0,\n    unk_id=1,\n    bos_id=2,\n    eos_id=3,\n\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model_file = \"/kaggle/input/iwslt15-clean-data/spm_en_vi.model\"\nmodel_file = \"/kaggle/input/iwslt-en-vi-2015/spm_en_vi.model\"\nif not os.path.exists(model_file):\n    print(\"no model found\")\nelse:\n    sp = spm.SentencePieceProcessor()\n    sp.load(model_file)\n    print(\"tokenizer loaded and ready2go!\")\n\n    print(f\"Kích thước Vocab: {sp.get_piece_size()}\")\n\n    text_test = \"Sỗ sã sẵ sì sĩ sí sỉ sặ sằ\"\n    print(f\"Test encode '{text_test}': {sp.encode_as_pieces(text_test)}\")\n    print(f\"Test decode: {sp.decode(sp.encode_as_ids(text_test))}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import random_split \n# Định nghĩa các ID đặc biệt khớp với lúc train SentencePiece\nPAD_ID = 0\nUNK_ID = 1\nBOS_ID = 2\nEOS_ID = 3\n\ndef real_sentence(text):\n    size_flag = len(text) > 4\n    return bool(re.search(r'\\w', text)) and size_flag\n\nclass IWSLTDataset(Dataset):\n    def __init__(self, src_file, tgt_file, clean=False):\n        self.src_lines = []\n        self.tgt_lines = []\n        \n        print(\"Đang đọc và lọc dữ liệu...\")\n        with open(src_file, 'r', encoding='utf-8') as fs, \\\n             open(tgt_file, 'r', encoding='utf-8') as ft:\n            \n            for s_line, t_line in zip(fs, ft):\n                if clean:\n                    s_clean = clean_sentence(s_line)\n                    t_clean = clean_sentence(t_line)\n                    s_clean = s_line.rstrip(\"\\n\")\n                    t_clean = t_line.rstrip(\"\\n\")\n                else:\n                    s_clean = s_line.rstrip(\"\\n\")\n                    t_clean = t_line.rstrip(\"\\n\")\n                \n                # Chỉ lấy khi cả 2 câu đều không rỗng sau khi clean\n                if real_sentence(s_clean) and real_sentence(t_clean):\n                    self.src_lines.append(s_clean)\n                    self.tgt_lines.append(t_clean)\n        \n        print(f\"Hoàn tất. Số cặp câu: {len(self.src_lines)}\")\n        \n    def __len__(self):\n        return len(self.src_lines)\n\n    def __getitem__(self, idx):\n        return self.src_lines[idx], self.tgt_lines[idx]\n\n\ndef collate_batch(batch):\n    \"\"\"\n    Hàm này xử lý một batch dữ liệu:\n    1. Tokenize text thành list of IDs.\n    2. Thêm BOS (Start) và EOS (End) tokens.\n    3. Pad (điền số 0) để các câu có độ dài bằng nhau.\n    \"\"\"\n    src_batch, tgt_batch = [], []\n    \n    for src_text, tgt_text in batch:\n        src_ids = [BOS_ID] + sp.encode_as_ids(src_text) + [EOS_ID]\n        tgt_ids = [BOS_ID] + sp.encode_as_ids(tgt_text) + [EOS_ID]\n        \n        src_batch.append(torch.tensor(src_ids, dtype=torch.long))\n        tgt_batch.append(torch.tensor(tgt_ids, dtype=torch.long))\n    \n    # Pad sequence: tạo tensor (batch_size x max_seq_len)\n    # use pytorch pad_sequence()\n    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=PAD_ID)\n    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_ID)\n    \n    return src_padded, tgt_padded\n\n\ntrain_src_file = \"/kaggle/input/iwslt-en-vi-2015/cleaned_train.en.txt\"\ntrain_tgt_file = \"/kaggle/input/iwslt-en-vi-2015/cleaned_train.vi.txt\"\n\n# clean_en_path = '/kaggle/working/cleaned_train.en.txt'\n# clean_vi_path = '/kaggle/working/cleaned_train.vi.txt'\n\n# Tạo Dataset\ndataset = IWSLTDataset(train_src_file, train_tgt_file)\n\ntotal_size = len(dataset)\nval_size = int(0.2 * total_size)\ntrain_size = total_size - val_size\n\ntrain_dataset, val_dataset = random_split(\n    dataset,\n    [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)  # explicit seed for reproducibility\n)\n\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,          # shuffle mỗi epoch?\n    collate_fn=collate_batch, \n    num_workers=2,         \n    pin_memory=True        # idk, optimization\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,             # DO NOT shuffle validation \n    collate_fn=collate_batch,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"Đã tạo DataLoader với {len(train_dataset)} cặp câu. và {len(val_dataset)} cặp câu\")\nprint(f\"Số lượng batch mỗi epoch: {len(train_dataloader)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Xây dựng Kiến trúc Transformer\n#TODO\n- [ ] rotational encoding\n- [ ] smoothing loss vs entropy loss\n- [x] RMSNorm","metadata":{"id":"ihodqwQPjkIH"}},{"cell_type":"code","source":"import copy\n# clone helper\ndef get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])","metadata":{"id":"jUOJTwFZSd0Q","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Attention, Positional Encoding","metadata":{}},{"cell_type":"code","source":"def attention(query, key, value, mask= None, dropout= None):\n    \"\"\"\n    query, key, value: (batch_size x heads x seq_len x d_k )\n    mask: ?\n    dropout: nn.Dropout module from whichever module using this function\n    returns attention output (batch_size x heads x seq_len x d_k)\n      and weights\n    \"\"\"\n\n    d_k = query.size(-1) # get head size\n    # score = query x key_transpose/ head_size\n    score = torch.matmul(query, key.transpose(-2, -1) / math.sqrt(d_k))\n\n    if mask is not None:\n        score = score.masked_fill(mask == 0, float('-inf')) # big negative\n\n    # softmax to probabilities\n    score = nn.functional.softmax(score, dim = -1)\n\n    if dropout is not None:\n        score = dropout(score)\n    output = torch.matmul(score, value) # (seq_len x seq_len) @ (seq_len x d_model), no transpose :)\n    return output, score # don't really need score","metadata":{"id":"nhguDcVF4LdX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) *\n                             -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# query, key, value: (batch_size x heads x seq_len x d_k )\n# those tensor shapes are killing me man\n\n\nclass RotaryPositionalEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n        assert self.dim % 2 == 0, \"RoPE dim must be even\"\n        theta = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('theta', theta, persistence=False) # if persistence isn't set to false, can't resume training \n        self.register_buffer('cos_sin', None, persistence=False)\n        self.max_seq_len = 0\n\n\n    def build_rope_cache(self, seq_len: int = 4096):\n        if self.cos_sin is not None and seq_len <= self.max_seq_len:\n            return\n        seq_idx = torch.arange(seq_len, device=self.theta.device, dtype= self.theta.dtype)\n        idx_theta = torch.einsum('i,j->ij', seq_idx, self.theta) # pos*theta\n        self.cos_sin = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim = -1)\n        self.max_seq_len = seq_len\n        \n    def forward(self, x):\n        seq_len = x.shape[-2] # batch x heads x seq_len x d_k, want seq_len\n        self.build_rope_cache(seq_len)\n\n        cos = self.cos_sin[:seq_len, :, 0].unsqueeze(0).unsqueeze(0)\n        sin = self.cos_sin[:seq_len, :, 1].unsqueeze(0).unsqueeze(0)\n        \n        x1, x2 = x.chunk(2, dim=-1)\n\n        return torch.cat(\n            [x1 * cos - x2 * sin,\n             x1 * sin + x2 * cos],\n            dim=-1\n        )","metadata":{"id":"30b5a9c6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n  # d_model = embedding dimensions\n  def __init__(self, h, d_model, rotary=None, dropout=0.1):\n    super(MultiHeadedAttention, self).__init__()\n    assert d_model % h == 0\n    self.d_k = d_model // h\n    self.h = h\n    self.linears = get_clones(nn.Linear(d_model, d_model), 4)\n    self.attn = None\n    self.dropout = nn.Dropout(p=dropout)\n    self.rotary = rotary \n\n  def forward(self, query, key, value, mask= None, return_attn=False):\n    \"\"\"\n    q,k,v: batch_size x seq_length x d_model\n    mask?\n    output: batch_size x seq_length x d_model\n    \"\"\"\n    if mask is not None:\n      # Same mask applied to all h heads.\n      mask = mask.unsqueeze(1)\n\n    nbatches = query.size(0)\n\n    query, key, value = [\n        lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n        for lin, x in zip(self.linears, (query, key, value))\n    ]\n      \n    if self.rotary is not None:\n        query = self.rotary(query)\n        key = self.rotary(key)\n\n    # attention on all the projected vectors in batch.\n    x, self.attn = attention(\n        query, key, value, mask=mask, dropout=self.dropout\n    )\n\n    #  \"Concat\" using a view and apply a final linear.\n    x = (\n        x.transpose(1, 2)\n        .contiguous()\n        .view(nbatches, -1, self.h * self.d_k)\n    )\n    del query\n    del key\n    del value\n    output = self.linears[-1](x)\n    if return_attn:\n        return output, self.attn\n    return output","metadata":{"id":"535210b6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Embedding b4 encoder","metadata":{}},{"cell_type":"code","source":"# embedding before encoder\n# lut = lookup table, x is the tensor to look up\n#\nclass Embedding(nn.Module):\n  def __init__(self, d_model, vocab):\n    super(Embedding, self).__init__()\n    self.lut = nn.Embedding(vocab, d_model)\n    self.d_model = d_model\n  def forward(self, x):\n    return self.lut(x) * math.sqrt(self.d_model) # why???","metadata":{"id":"JyM_yTwhR_2I","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Norm and residual","metadata":{}},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    # implement layer normalization\n    # eps for numerical stability\n    def __init__(self, features, eps=1e-6):\n      super(LayerNorm, self).__init__()\n      self.a_2 = nn.Parameter(torch.ones(features))\n      self.b_2 = nn.Parameter(torch.zeros(features))\n      self.eps = eps\n    def forward(self, x):\n      mean = x.mean(-1, keepdim=True)\n      std = x.std(-1, keepdim=True)\n      return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n","metadata":{"id":"JbdX7zRrCKxW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# epsilon for numerical stability (again)\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        # (batch, seq_len, dim)\n        # the mean of the square along the last dimension\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) # rsqrt is 1/sqrt(x)\n\n    def forward(self, x):\n        output = self._norm(x.float()).type_as(x) # safe for sqrt?\n        return output * self.weight","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SublayerConnection(nn.Module):\n  # residual + norm\n  # norm before or after? before!\n  \"\"\" connect sub layers \"\"\"\n  def __init__(self, size, dropout):\n    super(SublayerConnection, self).__init__()\n    self.norm = RMSNorm(size)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x, sublayer):\n    return x + self.dropout(sublayer(self.norm(x)))","metadata":{"id":"o5i-jDddCw7S","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n  # add non-linearities to attention\n  # just two linear transformation with an activation (ReLU)\n  # we can try using something else like SiLU, but can we justify it?\n  def __init__(self, d_model, d_ff, dropout=0.1):\n    super(PositionwiseFeedForward, self).__init__()\n    self.w_1 = nn.Linear(d_model, d_ff)\n    self.w_2 = nn.Linear(d_ff, d_model)\n    self.dropout = nn.Dropout(dropout)\n  def forward(self, x):\n    return self.w_2(self.dropout(self.w_1(x).relu()))","metadata":{"id":"CSAk8k0CEPV8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Encoder-Decoder","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    # nn.Module là lớp cơ sở cho tất cả các mạng neural trong PyTorch\n    \"\"\"multiple stacked layers of EncoderLayer\"\"\"\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = get_clones(layer, N)\n        self.norm = RMSNorm(layer.size)\n        \n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)","metadata":{"id":"XvFzc4GvyyKh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n  \"\"\"a layer of encoder\"\"\"\n  \"\"\"self attention then ffw\"\"\"\n  # size = d_model\n  # x is input?\n  def __init__(self, size, self_attn, ffw, dropout):\n    super(EncoderLayer, self).__init__()\n    self.self_attn = self_attn\n    self.ffw = ffw\n    self.sublayer = get_clones(SublayerConnection(size, dropout), 2)\n    self.size = size\n  def forward(self, x, mask):\n    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # q,k,v is the same!\n    return self.sublayer[1](x, lambda x: self.ffw(x))\n","metadata":{"id":"5tl2VykwFQax","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n  # self-explanatory\n  # needs mask for self-attention\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = get_clones(SublayerConnection(size, dropout), 3)\n    def forward(self, x, memory, src_mask, tgt_mask, return_attn=False):\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        \n        x2, attn = self.src_attn(x, memory, memory, src_mask, return_attn=True) # propagate attn to decoder layer\n        x = self.sublayer[1](x, lambda _: x2)\n        x = self.sublayer[2](x, self.feed_forward)\n        if return_attn:\n            return x, attn\n        return x","metadata":{"id":"t2FPSvKRFw6P","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# mask helper\ndef subsequent_mask(size):\n    \"Mask out future positions\"\n    return torch.tril(torch.ones(size, size, dtype=torch.bool)).unsqueeze(0)","metadata":{"id":"AW1-vCEsyg0g","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Decoder(nn.Module):\n  # cũng nhiều lớp decoder xếp chồng lên nhau\n    def __init__(self, layer, N):\n        super(Decoder, self).__init__()\n        self.layers = get_clones(layer, N)\n        self.norm = RMSNorm(layer.size)\n    def forward(self, x, memory, src_mask, tgt_mask, return_attn=False):\n        all_attn = []\n        for layer in self.layers:\n            if return_attn:\n                x, attn = layer(x, memory, src_mask, tgt_mask, return_attn)\n                all_attn.append(attn)\n            else:\n                x = layer(x, memory, src_mask, tgt_mask)\n        x = self.norm(x)\n        if return_attn:\n            return x, all_attn[-1] # last layer attention only\n        return x","metadata":{"id":"ZUwlg331FUHC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    \"\"\"\n    wrap the things together\n    \"\"\"\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        \"Take in and process masked src and target sequences.\"\n        return self.decode(self.encode(src, src_mask), src_mask,\n                            tgt, tgt_mask)\n\n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n\n    def decode(self, memory, src_mask, tgt, tgt_mask, return_attn=False):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask, return_attn=return_attn)","metadata":{"id":"0NPv9_KtGpjf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Generator(nn.Module):\n  \"\"\"final linear + softmax step\"\"\"\n  def __init__(self, d_model, vocab):\n      super(Generator, self).__init__()\n      self.proj = nn.Linear(d_model, vocab)\n  def forward(self, x):\n      return nn.functional.log_softmax(self.proj(x), dim=-1)","metadata":{"id":"nvNmddmjF1eH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Huấn luyện và Đánh giá\n#TODO\n- [ ] length penalty for beam search\n- [ ] step training, hyperparameter tuning\n- [ ] model ensemble","metadata":{"id":"FjtxPI7QHPBQ"}},{"cell_type":"code","source":"class Batch:\n    def __init__(self, src, tgt=None, pad=PAD_ID):  # 2 = <blank>\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)\n        if tgt is not None:\n            self.tgt = tgt[:, :-1]\n            self.tgt_y = tgt[:, 1:]\n            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n            self.ntokens = (self.tgt_y != pad).data.sum()\n\n    # hide the PADDING too\n    @staticmethod\n    def make_std_mask(tgt, pad):\n        \"Create a mask to hide padding and future words.\"\n        tgt_mask = (tgt != pad).unsqueeze(-2)\n        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n            tgt_mask.data\n        )\n        return tgt_mask","metadata":{"id":"WNZfc_as1uy0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_model(src_vocab, tgt_vocab, layers , h, d_model , d_ff , dropout = 0.1):\n  # just make a model :)\n  c = copy.deepcopy\n  # rotary_pe = RotaryPositionalEmbedding(d_model // h)\n  attn = MultiHeadedAttention(h, d_model) # Pass rotary_pe to attention\n  ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n  position = PositionalEncoding(d_model, dropout)\n  model = EncoderDecoder(\n    Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), layers),\n    Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), layers),\n    nn.Sequential(Embedding(d_model, src_vocab),c(position)),\n    nn.Sequential(Embedding(d_model, tgt_vocab),c(position)),\n    Generator(d_model, tgt_vocab),\n  )\n  for p in model.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n  return model","metadata":{"id":"FYoBxrN4HR8I","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_model_rotary(src_vocab, tgt_vocab, layers , h, d_model , d_ff , dropout = 0.1):\n  # just make a model :)\n  c = copy.deepcopy\n  rotary_pe = RotaryPositionalEmbedding(d_model // h)\n  attn = MultiHeadedAttention(h, d_model, rotary_pe) # Pass rotary_pe to attention\n  ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n  # position = PositionalEncoding(d_model, dropout)\n  model = EncoderDecoder(\n    Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), layers),\n    Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), layers),\n    Embedding(d_model, src_vocab),\n    Embedding(d_model, tgt_vocab),\n    Generator(d_model, tgt_vocab),\n  )\n  for p in model.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n  return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Beam search","metadata":{}},{"cell_type":"code","source":"# used in inference\ndef beam_search(model, memory, src_mask, start_symbol, end_symbol, max_len=100, beam_size=5, alpha= 0.6, beta=0.2, device='cpu'):\n    # memory: output of the encoder\n    # src_mask: source mask\n    # start_symbol: token for <s>\n    # end_symbol: token for </s>\n    # max_len: maximum length of generated sequence\n    # beam_size: number of candidates to keep at each step\n\n    def length_penalty(seq_len, alpha=0.6):\n        return ((5 + seq_len) / (5 + 1)) ** alpha\n    \n    model.eval()\n\n    # Initialize with the start symbol\n    ys = torch.full((1, 1), start_symbol, dtype=torch.long, device=device)\n\n    # Store (log_probability, sequence_so_far)\n    candidates = [(0.0, ys,  torch.zeros(memory.size(1), device=device))] # zero coverage vector at start\n\n    for _ in range(max_len - 1):\n        new_candidates = []\n        # Iterate through current best candidates\n        for log_prob, current_sequence, coverage in candidates:\n\n            # if the sequence already ended, keep it as is\n            if current_sequence[0, -1].item() == end_symbol:\n                new_candidates.append((log_prob, current_sequence, coverage))\n                continue\n\n            # target mask for the current sequence\n            tgt_mask = subsequent_mask(current_sequence.size(-1)).type_as(memory)\n\n            # Decode the next token\n            out, attn = model.decode(memory, src_mask, current_sequence, tgt_mask, return_attn=True)\n            step_attn = attn.mean(dim=1)[:, -1, :] #(batch, heads, tgt_len, src_len)\n            \n            prob = model.generator(out[:, -1]) # Get probabilities for the last token\n            log_probs = prob.log_softmax(dim=-1) # Convert to log probabilities\n\n            # Get top 'beam_size' next tokens and their log probabilities\n            top_k_log_probs, top_k_indices = log_probs.topk(beam_size)\n\n            for i in range(beam_size):\n                next_token_log_prob = top_k_log_probs[0, i].item()\n                next_token = top_k_indices[0, i].item()\n\n                extended_sequence = torch.cat(\n                    [current_sequence, torch.full((1, 1), next_token, dtype=torch.long, device=device)], dim=1\n                )\n                new_coverage = coverage + step_attn.squeeze(0) # accumulate average\n\n                len_penalty = length_penalty(extended_sequence.size(1), alpha)\n                coverage_penalty = beta * torch.sum(torch.log(torch.clamp(new_coverage, max=1.0)))\n                \n                # Extend the current sequence with the new token\n\n                new_score = (log_prob + next_token_log_prob) / len_penalty + coverage_penalty\n                new_candidates.append((new_score, extended_sequence, new_coverage))\n\n        # Sort all new candidates by new scoring!!! and select the top beam_size\n        candidates = sorted(new_candidates, key=lambda x: x[0], reverse=True)[:beam_size]\n\n        # If all best candidates have ended, stop early\n        if all(cand[1][0, -1].item() == end_symbol for cand in candidates):\n            break\n\n    # Return the best sequence (highest log probability)\n    return candidates[0][1].cpu().squeeze().tolist()\n","metadata":{"id":"n8D7aakj4c89","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rate(step, d_model, factor, warmup):\n    \"\"\"\n    we have to default the step to 1 for LambdaLR function\n    to avoid zero raising to negative power.\n    \"\"\"\n    if step == 0:\n        step = 1\n    return factor * (\n        d_model ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n    )","metadata":{"id":"aFvkdmPtjlsl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loss\n# is it self.cls - 2 or -1 ? \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, padding_idx = 0, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n        self.padding_idx = padding_idx\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 2))\n            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n            \n            true_dist[:, self.padding_idx] = 0\n            mask = torch.nonzero(target == self.padding_idx, as_tuple=False)\n            if mask.dim() > 0:\n                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","metadata":{"id":"I8wG2ybcn6W0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"tạo val dataset ở trên, k chạy cell này","metadata":{}},{"cell_type":"markdown","source":"## Hàm Train","metadata":{}},{"cell_type":"markdown","source":"- [ ] explain scheduler, đồ thị cho LR trong report!!!\n- [ ] tune hyperparameters from validation\n- [x] bỏ tối ưu độ dài câu (<150)\n- [ ] gradient accumulation?\n- [ ] gradient clipping?\n- [x] reload scaler?","metadata":{}},{"cell_type":"code","source":"# used below to save checkpoints\nsave_dir = \"/kaggle/working/checkpoints\"\nos.makedirs(save_dir, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n\n#validation cannot use smoothing\nval_criterion = nn.CrossEntropyLoss(\nignore_index=PAD_ID,\nreduction=\"mean\"   # IMPORTANT\n)\n\n\nHISTORY_PATH = \"/kaggle/working/history\"\nHISTORY_FILE = os.path.join(HISTORY_PATH, \"history.json\")\nos.makedirs(HISTORY_PATH, exist_ok= True)\ndef save_history(history, path=HISTORY_FILE):\n    with open(path, \"w\") as f:\n        json.dump(history, f, indent=2)\n\ndef train_with_validation_and_checkpointing(\n    model, train_dataloader, val_dataloader, criterion, optimizer, lr_scheduler, scaler,\n    device, pad_idx, sp_model, num_epochs, sample_sentence,\n    start_epoch=0, best_val_loss=float('inf')\n):\n    \"\"\"\n    train 1 epoch??? \n    \"\"\"\n\n    \n    model.train()\n    \n    # khởi tạo history để lưu kết quả\n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"val_ppl\": [],\n        \"epoch_time\": []\n    }\n\n    # gradient_norm = []\n    \n    model_internal = model.module if hasattr(model, 'module') else model\n    \n    print(f\"{'*'*3} Training from epoch {start_epoch + 1} {'*'*3}\")\n    for epoch in range(start_epoch, num_epochs):\n        print(f\"\\n--- Epoch {epoch + 1}/{num_epochs} ---\")\n        epoch_start = time.time()\n        model.train()\n        train_loss = 0.0\n        pbar = tqdm(train_dataloader, desc=\"Training\") # just prog. bar\n\n        ckpt_path = os.path.join(\n        save_dir,\n        f\"checkpoint_epoch_{epoch+1}.pth\"\n        ) \n        \n        for i, batch_data in enumerate(pbar):\n            src, tgt = batch_data[0].to(device), batch_data[1].to(device)\n            \n            batch = Batch(src, tgt, pad=pad_idx)\n            with torch.amp.autocast(device_type=\"cuda\"):\n            # with autocast():\n                out = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n                out = model_internal.generator(out)\n                loss = criterion(out.contiguous().view(-1, out.size(-1)), batch.tgt_y.contiguous().view(-1))\n            del out\n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n\n            # # testing gradient norm for learning with and without warmup\n            # scaler.unscale_(optimizer)\n            # total_norm = torch.norm(\n            #     torch.stack([\n            #         p.grad.detach().norm(2)\n            #         for p in model.parameters()\n            #         if p.grad is not None\n            #     ]),\n            #     2\n            # )\n            # gradient_norm.append(total_norm)\n\n            scaler.step(optimizer)\n            scaler.update()\n            lr_scheduler.step()\n            \n            train_loss += loss.item()\n            pbar.set_postfix({\"loss\": loss.item(), \"lr\": optimizer.param_groups[0][\"lr\"]})\n            \n        avg_train_loss = train_loss / len(train_dataloader)\n\n        avg_val_loss, val_ppl = evaluate_loss(model, val_dataloader, val_criterion, device, pad_idx)\n        \n        print(f\"KẾT QUẢ EPOCH {epoch+1}:\")\n        print(f\"Train Loss: {avg_train_loss:.4f}\")\n        print(f\"Val Loss:   {avg_val_loss:.4f} (Perplexity: {val_ppl:.2f})\")\n\n        if epoch % 2 == 0 or num_epochs - epoch <= 3:\n            torch.save({\n                'epoch': epoch + 1, # Lưu epoch kế tiếp sẽ chạy\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': lr_scheduler.state_dict(),\n                'scaler_state_dict' : scaler.state_dict(),\n                'best_val_loss': best_val_loss,\n            }, ckpt_path)\n        \n        if avg_val_loss < best_val_loss:\n            print(f\" Loss giảm ({best_val_loss:.4f} -> {avg_val_loss:.4f}). Lưu best checkpoint!\")\n            best_val_loss = avg_val_loss\n        else:\n            print(f\" Loss không giảm (Best: {best_val_loss:.4f}).\")\n            \n        # Lưu kết quả vào history\n        epoch_time = time.time() - epoch_start\n        history[\"epoch_time\"].append(epoch_time)\n        history[\"train_loss\"].append(avg_train_loss)\n        history[\"val_loss\"].append(avg_val_loss)\n        history[\"val_ppl\"].append(val_ppl)\n        save_history(history)\n        if sp_model:\n            try:\n                pred = translate_sentence(sample_sentence, model, sp_model, device)\n                print(f\"   - Dịch thử: {pred}\")\n            except: pass\n            \n    print(f\"\\nTraining hoàn tất! Best Val Loss: {best_val_loss:.4f}\")\n    return history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate \n#TODO\n- [x] change the names\n- [ ] more metrics? TER, METEOR, COMET, BERTscore?","metadata":{}},{"cell_type":"code","source":"import math\n\ndef evaluate_loss(model, val_dataloader, criterion, device, pad_idx = PAD_ID):\n    \"\"\"\n    Chạy model trên validation data để tính Loss và Perplexity và?\n    \"\"\"\n    model.eval() \n    total_loss = 0.0\n    model_internal = model.module if hasattr(model, 'module') else model\n    with torch.no_grad(): # ko gradient\n        for batch_data in val_dataloader:\n            src, tgt = batch_data[0].to(device), batch_data[1].to(device)\n            batch = Batch(src, tgt, pad=pad_idx)\n            \n            # fw patch\n            out = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n            out = model_internal.generator(out)\n            \n            # Tính Loss\n            loss = criterion(out.contiguous().view(-1, out.size(-1)), \n                             batch.tgt_y.contiguous().view(-1))\n\n            total_loss += loss.item()\n            \n    avg_loss = total_loss / len(val_dataloader)\n    \n    # Tính Perplexity (PPL) = exp(Loss)\n    # PPL càng thấp càng tốt\n    try:\n        ppl = math.exp(avg_loss)\n    except OverflowError:\n        ppl = float('inf')\n        \n    model.train() # failsafe về train\n    return avg_loss, ppl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\ndef calculate_scores(model, dataset, tokenizer, alpha, beta, device, num_samples=100):\n    \"\"\"\n    Tính BLEU Score và chrF trên một tập mẫu\n    Args:\n        num_samples: Số lượng câu muốn test (để 100-200 cho nhanh)\n    \"\"\"\n    print(f\"Đang tính BLEU score trên {num_samples} câu ngẫu nhiên...\")\n        \n    refs = []       # Danh sách câu gốc (Reference)\n    preds = []      # Danh sách câu máy dịch (Prediction)\n    \n    # 2. Lấy mẫu ngẫu nhiên từ dataset\n    # Nếu dataset nhỏ hơn num_samples thì lấy hết\n    indices = random.sample(range(len(dataset)), min(len(dataset), num_samples))\n    \n    model.eval()\n    print(f\"Model Device: {next(model.parameters())}\")    \n    print(f\"Kích thước Vocab: {tokenizer.get_piece_size()}\")\n\n    # 3. Chạy vòng lặp dịch\n    for i in tqdm(indices, desc=\"Translating\"):\n        src_text = dataset.src_lines[i] # Câu tiếng Anh\n        tgt_text = dataset.tgt_lines[i] # Câu tiếng Việt gốc (Đáp án)\n        try:\n            # Dịch\n            pred_text = translate_sentence(src_text, model, tokenizer, device, max_len = 100, alpha = alpha, beta = beta)\n            \n            preds.append(pred_text)\n            # BLEU yêu cầu reference là một list\n            # Ở đây ta chỉ có 1 đáp án nên để list chứa 1 string\n            refs.append([tgt_text]) \n            \n        except Exception as e:\n            print(f\"Lỗi ở index {i}: {e}\")\n            continue\n\n    bleu = sacrebleu.corpus_bleu(preds, refs)\n    chrf = sacrebleu.corpus_chrf(preds, refs)\n    \n    print(f\"BLEU score: {bleu.score:.2f}\")\n    print(f\"chrF score: {chrf.score:.2f}\")\n    \n    return {\n            \"bleu\": bleu.score,      # This is a float (0-100)\n            \"chrf\": chrf.score,      # This is a float (0-100)\n            \"predictions\": preds,    # Keep these for manual checking\n            \"references\": refs\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Translate a sentence with trained model and tokenizer.","metadata":{}},{"cell_type":"code","source":"# IMPORTANT: the tokenizer is from sentencepiece\ndef translate_sentence(sentence, model, tokenizer, device, max_len=100,alpha=0.6, beta=0.2):\n    model.eval()\n    \n    # tokenize câu đầu vào\n    # Thêm BOS và EOS (giống lúc train)\n    tokens = [BOS_ID] + tokenizer.encode_as_ids(sentence) + [EOS_ID]\n    src = torch.tensor(tokens).long().unsqueeze(0).to(device) # size: (1, seq_len)\n    \n    # tạo mask để bỏ qua padding\n    src_mask = (src != PAD_ID).unsqueeze(-2)\n    \n    # encode\n    with torch.no_grad():\n        memory = model.encode(src, src_mask)\n        output_ids = beam_search(model, memory, src_mask, BOS_ID, EOS_ID, max_len, beam_size=3, device=device, alpha=alpha, beta=beta)\n    \n    if EOS_ID in output_ids:\n        output_ids = output_ids[:output_ids.index(EOS_ID)]\n        \n    translation = tokenizer.decode(output_ids)\n    \n    model.train()\n    return translation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_checkpoint(model, optimizer, scheduler, scaler, filepath, device='cpu'):\n    start_epoch = 0\n    best_val_loss = float('inf')\n    \n    if os.path.exists(filepath):\n        print(f\"--- Đang tải checkpoint từ: {filepath} ---\")\n        checkpoint = torch.load(filepath, map_location=device)\n        \n        # reload các thứ (cần reload gradscaler ko?)\n        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n        \n        # Lấy thông tin epoch và loss để train tiếp\n        start_epoch = checkpoint['epoch']\n        best_val_loss = checkpoint['best_val_loss']\n        \n        print(f\" train tiếp từ Epoch {start_epoch + 1}.\")\n        print(f\" Best Val Loss đã ghi nhận: {best_val_loss:.4f}\")\n        print(f\"Learning Rate hiện tại: {optimizer.param_groups[0]['lr']:.6f}\")\n    else:\n        print(\" ko tìm thấy checkpoint, train từ đầu\")\n        \n    return start_epoch, best_val_loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot các thứ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_metrics(history):\n    \"\"\"\n    Vẽ đồ thị Train Loss, Val Loss, và Val Perplexity.\n    \"\"\"\n    train_loss = history['train_loss']\n    val_loss = history['val_loss']\n    val_ppl = history['val_ppl']\n    epochs = range(1, len(train_loss) + 1)\n    \n    plt.figure(figsize=(14, 5))\n    \n    # Loss \n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # Perplexity\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, val_ppl, 'go-', label='Validation Perplexity')\n    plt.title('Validation Perplexity')\n    plt.xlabel('Epochs')\n    plt.ylabel('Perplexity')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(\"/kaggle/working/metric_graph.png\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Khởi tạo model, optimizer, scheduler, train","metadata":{}},{"cell_type":"code","source":"# LabelSmoothingLoss cần input đúng kích thước vocab\ncriterion = LabelSmoothingLoss(classes=VOCAB_SIZE, padding_idx=PAD_ID, smoothing=0.0)\n\nmodel = make_model_rotary(src_vocab=VOCAB_SIZE, tgt_vocab=VOCAB_SIZE, \n                   layers=LAYERS, h=H, d_model=D_MODEL, d_ff=D_FF, dropout=0.1)\n\n# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs\")\n#     model = nn.DataParallel(model)\n    \n# add vào device or sth idk\nmodel.to(device)\n\nprint(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_parameters(model):\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            print(f\"{name:50} | {param.numel():>10}\")\n\ncount_parameters(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"cell này để train","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1.0, betas=(0.9, 0.98), eps=1e-9)\nlr_scheduler = LambdaLR(optimizer=optimizer,\n    lr_lambda=lambda step: rate(step, d_model=D_MODEL, factor=1, warmup=WARMUP))\nscaler = torch.amp.GradScaler(\"cuda\")\n\nstart_epoch = 0\n\ncheckpoint_path = \"/kaggle/working/checkpoints/checkpoint_epoch_2.pth\"\nstart_epoch, best_val_loss = load_checkpoint(model, optimizer, lr_scheduler, scaler, checkpoint_path, device)\n# train từ đầu hoặc train tiếp \nhistory = None\ntry:\n    history = train_with_validation_and_checkpointing(\n        model=model,\n        train_dataloader=train_dataloader,\n        val_dataloader=val_dataloader,\n        criterion=criterion,\n        optimizer=optimizer,         \n        lr_scheduler=lr_scheduler,\n        scaler = scaler,\n        device=device,\n        pad_idx=PAD_ID,\n        sp_model=sp,\n        num_epochs=20,               \n        sample_sentence=\"The weapon of criticism obviously cannot replace the criticism of weapons.\",\n        start_epoch=start_epoch,     \n        best_val_loss=best_val_loss  \n    )\nexcept KeyboardInterrupt:\n    print(\"Đang train thì bị ép dừng :(((\")\nfinally:\n    if history is not None:\n        plot_metrics(history)\n    else:\n        print(\"no history found. training failed?\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom tabulate import tabulate\n\nwith open('/kaggle/working/history/history.json', 'r') as f:\n    data = json.load(f)\n\ntrain_loss = data.get(\"train_loss\", [])\nval_loss = data.get(\"val_loss\", [])\nval_ppl = data.get(\"val_ppl\", [])\n\nhistory = {\n    \"train_loss\": train_loss,\n    \"val_loss\": val_loss,\n    \"val_ppl\": val_ppl,\n}\nplot_metrics(history)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# real_history = history[0]\n\n# gradient_norm = history[1]\n\n# gradient = []\n# gradient.extend(t.cpu() for t in gradient_norm)\n\n# plt.figure(figsize=(12, 5))\n# plt.plot(gradient, color='tab:red', linewidth=1, label='Total Gradient Norm')\n\n# # plt.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Clip Threshold (1.0)')\n\n# plt.title('Gradient Norm per Step')\n# plt.xlabel('Training Step (Batch)')\n# plt.ylabel('Norm Value')\n# plt.grid(True, alpha=0.3)\n# plt.legend()\n\n# # plt.yscale('log') \n# plt.savefig(\"/kaggle/working/grad_norm_4kwarmup.png\")\n\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# serializable_list = [t.detach().cpu().item() for t in gradient]\n\n# with open(\"/kaggle/working/gradient_norms.json\", \"w\") as f:\n#     json.dump(serializable_list, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# %whos","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model = make_model_rotary(src_vocab = VOCAB_SIZE, tgt_vocab= VOCAB_SIZE, layers=LAYERS , h=H, d_model=D_MODEL , d_ff=D_FF , dropout = 0.1)\ntest_optimizer = torch.optim.AdamW(test_model.parameters(), lr=1e-4)\ntest_scheduler = torch.optim.lr_scheduler.StepLR(test_optimizer, step_size=1)\ntest_scaler = torch.amp.GradScaler(\"cuda\")\nfilepath = \"/kaggle/working/checkpoints/checkpoint_epoch_20.pth\"\nload_checkpoint(test_model, test_optimizer, test_scheduler, test_scaler, filepath, device)\n# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs\")\n#     test_model = nn.DataParallel(test_model)\ntest_model.to(device)\ntest_model.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_src_file = \"/kaggle/input/iwslt15-englishvietnamese/IWSLT'15 en-vi/tst2013.en.txt\"\ntest_tgt_file = \"/kaggle/input/iwslt15-englishvietnamese/IWSLT'15 en-vi/tst2013.vi.txt\"\n\nen_test = load_file(test_src_file)\nvi_test = load_file(test_tgt_file)\n\nwith open(\"cleaned_test.vi.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.writelines(line + '\\n' for line in vi_test)\nwith open(\"cleaned_test.en.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.writelines(line + '\\n' for line in en_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_test_vi = \"/kaggle/input/iwslt-en-vi-2015/cleaned_test.vi_2013.txt\"\nclean_test_en = \"/kaggle/input/iwslt-en-vi-2015/cleaned_test.en_2013.txt\"\n\ntest_dataset = IWSLTDataset(clean_test_en, clean_test_vi)\n\nresults = calculate_scores(\n    model=test_model, \n    dataset=test_dataset, \n    tokenizer=sp,\n    alpha = 0.3,\n    beta = 0.2,\n    device=\"cuda\", \n    num_samples=len(test_dataset),\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Nén toàn bộ file trong working thành output.zip\nos.system(\"zip -r output.zip .\")\n\nfrom IPython.display import FileLink\nprint(\"Bấm vào đây để tải trọn bộ (ZIP):\")\nFileLink(r'output.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}