{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14071510,"sourceType":"datasetVersion","datasetId":8957065},{"sourceId":687860,"sourceType":"modelInstanceVersion","modelInstanceId":521643,"modelId":535805},{"sourceId":689426,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":522539,"modelId":536522}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom collections import Counter\nfrom torch.utils.data import random_split, DataLoader\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torchmetrics.text import BLEUScore\nimport spacy\nfrom typing import List","metadata":{"id":"rLqUt6MOliqk","executionInfo":{"status":"ok","timestamp":1765818974750,"user_tz":-420,"elapsed":1354,"user":{"displayName":"23020079 B√πi An Huy","userId":"04676746815628624144"}},"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:54:04.639561Z","iopub.execute_input":"2025-12-16T21:54:04.639972Z","iopub.status.idle":"2025-12-16T21:54:04.646226Z","shell.execute_reply.started":"2025-12-16T21:54:04.639942Z","shell.execute_reply":"2025-12-16T21:54:04.645098Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# Load Train data","metadata":{}},{"cell_type":"code","source":"import sentencepiece as spm\nimport os\n\n# 1. ƒê∆∞·ªùng d·∫´n file model\nmodel_file = \"/kaggle/input/fist-model-10-ep/pytorch/default/1/spm_en_vi.model\"\n\nsp = spm.SentencePieceProcessor()\nsp.load(model_file)\nprint(\">>> ƒê√£ load Tokenizer th√†nh c√¥ng!\")\n\nprint(f\"K√≠ch th∆∞·ªõc Vocab: {sp.get_piece_size()}\")\n\n# 5. Test th·ª≠\ntext_test = \"Hello Vietnam\"\nprint(f\"Test encode '{text_test}': {sp.encode_as_ids(text_test)}\")\nprint(f\"Test decode: {sp.decode(sp.encode_as_ids(text_test))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:04.622684Z","iopub.execute_input":"2025-12-16T21:37:04.623273Z","iopub.status.idle":"2025-12-16T21:37:04.690049Z","shell.execute_reply.started":"2025-12-16T21:37:04.623235Z","shell.execute_reply":"2025-12-16T21:37:04.688927Z"}},"outputs":[{"name":"stdout","text":">>> ƒê√£ load Tokenizer th√†nh c√¥ng!\nK√≠ch th∆∞·ªõc Vocab: 16000\nTest encode 'Hello Vietnam': [11183, 9082]\nTest decode: Hello Vietnam\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader, Subset\n\n# ƒê·ªãnh nghƒ©a c√°c ID ƒë·∫∑c bi·ªát kh·ªõp v·ªõi l√∫c train SentencePiece\nPAD_ID = 0\nUNK_ID = 1\nBOS_ID = 2\nEOS_ID = 3\n\nclass IWSLTDataset(Dataset):\n    def __init__(self, src_file, tgt_file):\n        self.src_lines = []\n        self.tgt_lines = []\n        \n        print(\"ƒêang ƒë·ªçc v√† l·ªçc d·ªØ li·ªáu...\")\n        with open(src_file, 'r', encoding='utf-8') as fs, \\\n             open(tgt_file, 'r', encoding='utf-8') as ft:\n            \n            # D√πng zip ƒë·ªÉ ƒë·ªçc song song 2 file c√πng l√∫c\n            # N·∫øu 2 file l·ªách d√≤ng g·ªëc, zip s·∫Ω t·ª± d·ª´ng ·ªü file ng·∫Øn h∆°n -> Tr√°nh crash\n            for s_line, t_line in zip(fs, ft):\n                s_clean = s_line.strip()\n                t_clean = t_line.strip()\n                \n                # Logic quan tr·ªçng: Ch·ªâ l·∫•y khi C·∫¢ 2 ƒë·ªÅu c√≥ d·ªØ li·ªáu\n                if s_clean and t_clean:\n                    self.src_lines.append(s_clean)\n                    self.tgt_lines.append(t_clean)\n        \n        print(f\"Ho√†n t·∫•t load data. S·ªë c·∫∑p c√¢u h·ª£p l·ªá: {len(self.src_lines)}\")\n\n    def __len__(self):\n        return len(self.src_lines)\n\n    def __getitem__(self, idx):\n        return self.src_lines[idx], self.tgt_lines[idx]\n\n\ndef collate_batch(batch):\n    \"\"\"\n    H√†m n√†y x·ª≠ l√Ω m·ªôt batch d·ªØ li·ªáu:\n    1. Tokenize text th√†nh list of IDs.\n    2. Th√™m BOS (Start) v√† EOS (End) tokens.\n    3. Pad (ƒëi·ªÅn s·ªë 0) ƒë·ªÉ c√°c c√¢u c√≥ ƒë·ªô d√†i b·∫±ng nhau.\n    \"\"\"\n    src_batch, tgt_batch = [], []\n    \n    for src_text, tgt_text in batch:\n        # Tokenize v√† th√™m BOS/EOS\n        src_ids = [BOS_ID] + sp.encode_as_ids(src_text) + [EOS_ID]\n        tgt_ids = [BOS_ID] + sp.encode_as_ids(tgt_text) + [EOS_ID]\n        \n        src_batch.append(torch.tensor(src_ids, dtype=torch.long))\n        tgt_batch.append(torch.tensor(tgt_ids, dtype=torch.long))\n    \n    # Pad sequence: t·∫°o tensor h√¨nh ch·ªØ nh·∫≠t (Batch_Size, Max_Len)\n    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=PAD_ID)\n    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_ID)\n    \n    return src_padded, tgt_padded\n\n# --- C·∫§U H√åNH V√Ä T·∫†O DATALOADER ---\n\n# ƒê∆∞·ªùng d·∫´n file (s·ª≠ d·ª•ng file b·∫°n ƒë√£ t·∫°o ra ·ªü cell ƒë·∫ßu ti√™n)\ntrain_src_file = \"/kaggle/input/medicaldataset-vlsp/MedicalDataset_VLSP/train.en.txt\"\ntrain_tgt_file = \"/kaggle/input/medicaldataset-vlsp/MedicalDataset_VLSP/train.vi.txt\"\n\n# T·∫°o Dataset\nfull_train_dataset = Subset(IWSLTDataset(train_src_file, train_tgt_file),range(5000))\n\n# T·∫°o DataLoader\nBATCH_SIZE = 16 # B·∫°n c√≥ th·ªÉ gi·∫£m xu·ªëng 16 n·∫øu b·ªã tr√†n RAM (OOM)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:58:07.100351Z","iopub.execute_input":"2025-12-16T21:58:07.100973Z","iopub.status.idle":"2025-12-16T21:58:08.971997Z","shell.execute_reply.started":"2025-12-16T21:58:07.100923Z","shell.execute_reply":"2025-12-16T21:58:08.969233Z"}},"outputs":[{"name":"stdout","text":"ƒêang ƒë·ªçc v√† l·ªçc d·ªØ li·ªáu...\nHo√†n t·∫•t load data. S·ªë c·∫∑p c√¢u h·ª£p l·ªá: 500000\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# 1. T√≠nh to√°n k√≠ch th∆∞·ªõc 90% train, 10% validation\nval_size = int(len(full_train_dataset) * 0.1)\ntrain_size = len(full_train_dataset) - val_size\n\n# 2. T√°ch dataset\n# D√πng manual_seed ƒë·ªÉ ƒë·∫£m b·∫£o l·∫ßn n√†o ch·∫°y c≈©ng chia y h·ªát nhau\ntrain_subset, val_subset = random_split(\n    full_train_dataset, \n    [train_size, val_size],\n    generator=torch.Generator().manual_seed(42) \n)\n\n# 3. T·∫°o DataLoader t·ª´ c√°c t·∫≠p con\ntrain_loader = DataLoader(train_subset,num_workers=2,\n                          batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=collate_batch)\nval_loader = DataLoader(val_subset, batch_size=BATCH_SIZE,num_workers=2,   \n                        shuffle=False, collate_fn=collate_batch)\n\nprint(f\"Chia xong: {len(train_subset)} c√¢u train, {len(val_subset)} c√¢u val.\")\nprint(f\"-> S·∫µn s√†ng d√πng `train_loader` v√† `val_loader` ƒë·ªÉ hu·∫•n luy·ªán.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:58:10.503696Z","iopub.execute_input":"2025-12-16T21:58:10.505071Z","iopub.status.idle":"2025-12-16T21:58:10.817909Z","shell.execute_reply.started":"2025-12-16T21:58:10.505030Z","shell.execute_reply":"2025-12-16T21:58:10.816557Z"}},"outputs":[{"name":"stdout","text":"Chia xong: 4500 c√¢u train, 500 c√¢u val.\n-> S·∫µn s√†ng d√πng `train_loader` v√† `val_loader` ƒë·ªÉ hu·∫•n luy·ªán.\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"# X√¢y d·ª±ng Ki·∫øn tr√∫c Transformer","metadata":{"id":"ihodqwQPjkIH"}},{"cell_type":"code","source":"import copy\n# clone helper\ndef get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])","metadata":{"id":"jUOJTwFZSd0Q","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.494486Z","iopub.execute_input":"2025-12-16T21:37:06.494712Z","iopub.status.idle":"2025-12-16T21:37:06.500004Z","shell.execute_reply.started":"2025-12-16T21:37:06.494693Z","shell.execute_reply":"2025-12-16T21:37:06.498736Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def attention(query, key, value, mask= None, dropout= None):\n    \"\"\"\n    query, key, value: (batch_size x heads x seq_len x d_k )\n    mask: ?\n    dropout: nn.Dropout module from whichever module using this function\n    returns attention output (batch_size x heads x seq_len x d_k)\n      and weights\n    \"\"\"\n\n    d_k = query.size(-1) # get head size\n    # score = query x key_transpose/ head_size\n    score = torch.matmul(query, key.transpose(-2, -1) / math.sqrt(d_k))\n\n    if mask is not None:\n        score = score.masked_fill(mask == 0, float('-inf')) # big negative\n\n    # softmax to probabilities\n    score = nn.functional.softmax(score, dim = -1)\n\n    if dropout is not None:\n        score = dropout(score)\n    output = torch.matmul(score, value) # (seq_len x seq_len) @ (seq_len x d_model), no transpose :)\n    return output, score # don't really need score","metadata":{"id":"nhguDcVF4LdX","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.500909Z","iopub.execute_input":"2025-12-16T21:37:06.501191Z","iopub.status.idle":"2025-12-16T21:37:06.519262Z","shell.execute_reply.started":"2025-12-16T21:37:06.501169Z","shell.execute_reply":"2025-12-16T21:37:06.518370Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) *\n                             -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.520179Z","iopub.execute_input":"2025-12-16T21:37:06.520526Z","iopub.status.idle":"2025-12-16T21:37:06.537218Z","shell.execute_reply.started":"2025-12-16T21:37:06.520496Z","shell.execute_reply":"2025-12-16T21:37:06.536219Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class RotaryPositionalEmbedding(nn.Module):\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.dim = dim\n        self.base = base\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n    def forward(self, seq_len, *, device=None, dtype=None):\n        if device is None:\n            device = self.inv_freq.device\n        t = torch.arange(seq_len, device=device, dtype=dtype)\n        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n        # Complex numbers for rotation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        return emb.cos(), emb.sin()\n\ndef apply_rotary_pos_emb(q, k, freqs):\n    # freqs contains (cos, sin) values\n    cos, sin = freqs\n\n    # Reshape cos and sin for broadcasting: (1, 1, seq_len, dim)\n    # q, k shape: (batch_size, num_heads, seq_len, dim_per_head)\n    cos = cos.unsqueeze(0).unsqueeze(0)\n    sin = sin.unsqueeze(0).unsqueeze(0)\n\n    # Apply rotation to query and key using complex number multiplication equivalent\n    # [x0, x1, x2, x3, ...] -> [x0*cos - x1*sin, x0*sin + x1*cos, x2*cos - x3*sin, ...]\n    q_rot = q * cos - rotate_half(q) * sin\n    k_rot = k * cos - rotate_half(k) * sin\n\n    return q_rot, k_rot\n\ndef rotate_half(x):\n    # Rotates the second half of the embedding dimension by 180 degrees.\n    x1, x2 = x.chunk(2, dim=-1)\n    return torch.cat((-x2, x1), dim=-1)","metadata":{"id":"30b5a9c6","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.538149Z","iopub.execute_input":"2025-12-16T21:37:06.538770Z","iopub.status.idle":"2025-12-16T21:37:06.560924Z","shell.execute_reply.started":"2025-12-16T21:37:06.538743Z","shell.execute_reply":"2025-12-16T21:37:06.559732Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n  # d_model = embedding dimensions\n  def __init__(self, h, d_model, dropout=0.1):\n    super(MultiHeadedAttention, self).__init__()\n    assert d_model % h == 0\n    self.d_k = d_model // h\n    self.h = h\n    self.linears = get_clones(nn.Linear(d_model, d_model), 4)\n    self.attn = None\n    self.dropout = nn.Dropout(p=dropout)\n    # self.rotary = rotary # Store the RoPE instance\n\n  def forward(self, query, key, value, mask= None):\n    \"\"\"\n    q,k,v: batch_size x seq_length x d_model\n    mask?\n    output: batch_size x seq_length x d_model\n    \"\"\"\n    if mask is not None:\n      # Same mask applied to all h heads.\n      mask = mask.unsqueeze(1)\n\n    nbatches = query.size(0)\n\n    query, key, value = [\n        lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n        for lin, x in zip(self.linears, (query, key, value))\n    ]\n    # Apply attention on all the projected vectors in batch.\n    x, self.attn = attention(\n        query, key, value, mask=mask, dropout=self.dropout\n    )\n\n    #  \"Concat\" using a view and apply a final linear.\n    x = (\n        x.transpose(1, 2)\n        .contiguous()\n        .view(nbatches, -1, self.h * self.d_k)\n    )\n    del query\n    del key\n    del value\n    return self.linears[-1](x)","metadata":{"id":"535210b6","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.562044Z","iopub.execute_input":"2025-12-16T21:37:06.562441Z","iopub.status.idle":"2025-12-16T21:37:06.586665Z","shell.execute_reply.started":"2025-12-16T21:37:06.562407Z","shell.execute_reply":"2025-12-16T21:37:06.585786Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# embedding before encoder\n# lut = lookup table, x is the tensor to look up\n#\nclass Embedding(nn.Module):\n  def __init__(self, d_model, vocab):\n    super(Embedding, self).__init__()\n    self.lut = nn.Embedding(vocab, d_model)\n    self.d_model = d_model\n  def forward(self, x):\n    return self.lut(x) * math.sqrt(self.d_model) # why???","metadata":{"id":"JyM_yTwhR_2I","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.589483Z","iopub.execute_input":"2025-12-16T21:37:06.589760Z","iopub.status.idle":"2025-12-16T21:37:06.608491Z","shell.execute_reply.started":"2025-12-16T21:37:06.589738Z","shell.execute_reply":"2025-12-16T21:37:06.607450Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    # implement layer normalization\n    # eps for numerical stability\n    def __init__(self, features, eps=1e-6):\n      super(LayerNorm, self).__init__()\n      self.a_2 = nn.Parameter(torch.ones(features))\n      self.b_2 = nn.Parameter(torch.zeros(features))\n      self.eps = eps\n    def forward(self, x):\n      mean = x.mean(-1, keepdim=True)\n      std = x.std(-1, keepdim=True)\n      return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n","metadata":{"id":"JbdX7zRrCKxW","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.609590Z","iopub.execute_input":"2025-12-16T21:37:06.609932Z","iopub.status.idle":"2025-12-16T21:37:06.626365Z","shell.execute_reply.started":"2025-12-16T21:37:06.609902Z","shell.execute_reply":"2025-12-16T21:37:06.625300Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class SublayerConnection(nn.Module):\n  # residual + norm\n  # norm before or after? before!\n  \"\"\" connect sub layers \"\"\"\n  def __init__(self, size, dropout):\n    super(SublayerConnection, self).__init__()\n    self.norm = LayerNorm(size)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x, sublayer):\n    return x + self.dropout(sublayer(self.norm(x)))","metadata":{"id":"o5i-jDddCw7S","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.627118Z","iopub.execute_input":"2025-12-16T21:37:06.627448Z","iopub.status.idle":"2025-12-16T21:37:06.648676Z","shell.execute_reply.started":"2025-12-16T21:37:06.627421Z","shell.execute_reply":"2025-12-16T21:37:06.647685Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n  # add non-linearities to attention\n  # just two linear transformation with an activation (ReLU)\n  # we can try using something else like SiLU, but can we justify it?\n  def __init__(self, d_model, d_ff, dropout=0.1):\n    super(PositionwiseFeedForward, self).__init__()\n    self.w_1 = nn.Linear(d_model, d_ff)\n    self.w_2 = nn.Linear(d_ff, d_model)\n    self.dropout = nn.Dropout(dropout)\n  def forward(self, x):\n    return self.w_2(self.dropout(self.w_1(x).relu()))","metadata":{"id":"CSAk8k0CEPV8","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.649883Z","iopub.execute_input":"2025-12-16T21:37:06.650236Z","iopub.status.idle":"2025-12-16T21:37:06.692263Z","shell.execute_reply.started":"2025-12-16T21:37:06.650205Z","shell.execute_reply":"2025-12-16T21:37:06.691338Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class Encoder(nn.Module):\n    # nn.Module l√† l·ªõp c∆° s·ªü cho t·∫•t c·∫£ c√°c m·∫°ng neural trong PyTorch\n    \"\"\"multiple stacked layers of EncoderLayer\"\"\"\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = get_clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n        \n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)","metadata":{"id":"XvFzc4GvyyKh","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.693353Z","iopub.execute_input":"2025-12-16T21:37:06.693683Z","iopub.status.idle":"2025-12-16T21:37:06.712516Z","shell.execute_reply.started":"2025-12-16T21:37:06.693655Z","shell.execute_reply":"2025-12-16T21:37:06.711585Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n  \"\"\"a layer of encoder\"\"\"\n  \"\"\"self attention then ffw\"\"\"\n  # size = d_model\n  # x is input?\n  def __init__(self, size, self_attn, ffw, dropout):\n    super(EncoderLayer, self).__init__()\n    self.self_attn = self_attn\n    self.ffw = ffw\n    self.sublayer = get_clones(SublayerConnection(size, dropout), 2)\n    self.size = size\n  def forward(self, x, mask):\n    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # q,k,v is the same!\n    return self.sublayer[1](x, lambda x: self.ffw(x))\n","metadata":{"id":"5tl2VykwFQax","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.713456Z","iopub.execute_input":"2025-12-16T21:37:06.713726Z","iopub.status.idle":"2025-12-16T21:37:06.732527Z","shell.execute_reply.started":"2025-12-16T21:37:06.713698Z","shell.execute_reply":"2025-12-16T21:37:06.731499Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n  # self-explanatory\n  # needs mask for self-attention\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = get_clones(SublayerConnection(size, dropout), 3)\n    def forward(self, x, memory, src_mask, tgt_mask):\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n        return self.sublayer[2](x, self.feed_forward)","metadata":{"id":"t2FPSvKRFw6P","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.733529Z","iopub.execute_input":"2025-12-16T21:37:06.733935Z","iopub.status.idle":"2025-12-16T21:37:06.765650Z","shell.execute_reply.started":"2025-12-16T21:37:06.733904Z","shell.execute_reply":"2025-12-16T21:37:06.764718Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# mask helper\ndef subsequent_mask(size):\n    \"Mask out future positions\"\n    return torch.tril(torch.ones(size, size, dtype=torch.bool)).unsqueeze(0)","metadata":{"id":"AW1-vCEsyg0g","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.766528Z","iopub.execute_input":"2025-12-16T21:37:06.766847Z","iopub.status.idle":"2025-12-16T21:37:06.782952Z","shell.execute_reply.started":"2025-12-16T21:37:06.766825Z","shell.execute_reply":"2025-12-16T21:37:06.781954Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class Decoder(nn.Module):\n  # c≈©ng nhi·ªÅu l·ªõp decoder x·∫øp ch·ªìng l√™n nhau\n    def __init__(self, layer, N):\n        super(Decoder, self).__init__()\n        self.layers = get_clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)","metadata":{"id":"ZUwlg331FUHC","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.783996Z","iopub.execute_input":"2025-12-16T21:37:06.784492Z","iopub.status.idle":"2025-12-16T21:37:06.801357Z","shell.execute_reply.started":"2025-12-16T21:37:06.784460Z","shell.execute_reply":"2025-12-16T21:37:06.800358Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    \"\"\"\n    wrap the things together\n    \"\"\"\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        \"Take in and process masked src and target sequences.\"\n        return self.decode(self.encode(src, src_mask), src_mask,\n                            tgt, tgt_mask)\n\n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)","metadata":{"id":"0NPv9_KtGpjf","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.802352Z","iopub.execute_input":"2025-12-16T21:37:06.802757Z","iopub.status.idle":"2025-12-16T21:37:06.823182Z","shell.execute_reply.started":"2025-12-16T21:37:06.802728Z","shell.execute_reply":"2025-12-16T21:37:06.822165Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class Generator(nn.Module):\n  \"\"\"final linear + softmax step\"\"\"\n  def __init__(self, d_model, vocab):\n      super(Generator, self).__init__()\n      self.proj = nn.Linear(d_model, vocab)\n  def forward(self, x):\n      return nn.functional.log_softmax(self.proj(x), dim=-1)","metadata":{"id":"nvNmddmjF1eH","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.824259Z","iopub.execute_input":"2025-12-16T21:37:06.824685Z","iopub.status.idle":"2025-12-16T21:37:06.845754Z","shell.execute_reply.started":"2025-12-16T21:37:06.824655Z","shell.execute_reply":"2025-12-16T21:37:06.844694Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Hu·∫•n luy·ªán v√† ƒê√°nh gi√°","metadata":{"id":"FjtxPI7QHPBQ"}},{"cell_type":"code","source":"class Batch:\n    def __init__(self, src, tgt=None, pad=PAD_ID):  # 2 = <blank>\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)\n        if tgt is not None:\n            self.tgt = tgt[:, :-1]\n            self.tgt_y = tgt[:, 1:]\n            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n            self.ntokens = (self.tgt_y != pad).data.sum()\n\n    # hide the PADDING too\n    @staticmethod\n    def make_std_mask(tgt, pad):\n        \"Create a mask to hide padding and future words.\"\n        tgt_mask = (tgt != pad).unsqueeze(-2)\n        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n            tgt_mask.data\n        )\n        return tgt_mask","metadata":{"id":"WNZfc_as1uy0","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.846988Z","iopub.execute_input":"2025-12-16T21:37:06.847378Z","iopub.status.idle":"2025-12-16T21:37:06.864913Z","shell.execute_reply.started":"2025-12-16T21:37:06.847348Z","shell.execute_reply":"2025-12-16T21:37:06.863823Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def make_model(src_vocab, tgt_vocab, layers , h, d_model , d_ff , dropout = 0.1):\n  # just make a model :)\n  c = copy.deepcopy\n  # rotary_pe = RotaryPositionalEmbedding(d_model // h)\n  attn = MultiHeadedAttention(h, d_model) # Pass rotary_pe to attention\n  ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n  position = PositionalEncoding(d_model, dropout)\n  model = EncoderDecoder(\n    Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), layers),\n    Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), layers),\n    nn.Sequential(Embedding(d_model, src_vocab),c(position)),\n    nn.Sequential(Embedding(d_model, tgt_vocab),c(position)),\n    Generator(d_model, tgt_vocab),\n  )\n  for p in model.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n  return model","metadata":{"id":"FYoBxrN4HR8I","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.865853Z","iopub.execute_input":"2025-12-16T21:37:06.866301Z","iopub.status.idle":"2025-12-16T21:37:06.882701Z","shell.execute_reply.started":"2025-12-16T21:37:06.866266Z","shell.execute_reply":"2025-12-16T21:37:06.881665Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# used in inference\ndef beam_search(model, memory, src_mask, start_symbol, end_symbol, max_len=100, beam_size=5, device='cpu'):\n    # memory: output of the encoder\n    # src_mask: source mask\n    # start_symbol: token for <s>\n    # end_symbol: token for </s>\n    # max_len: maximum length of generated sequence\n    # beam_size: number of candidates to keep at each step\n\n    # Ensure the model is in evaluation mode\n    model.eval()\n\n    # Initialize with the start symbol\n    ys = torch.full((1, 1), start_symbol, dtype=torch.long, device=device)\n\n    # Store (log_probability, sequence_so_far)\n    candidates = [(0.0, ys)]\n\n    for _ in range(max_len - 1):\n        new_candidates = []\n        # Iterate through current best candidates\n        for log_prob, current_sequence in candidates:\n\n            # If the sequence already ended, keep it as is\n            if current_sequence[0, -1].item() == end_symbol:\n                new_candidates.append((log_prob, current_sequence))\n                continue\n\n            # Create target mask for the current sequence\n            tgt_mask = subsequent_mask(current_sequence.size(-1)).type_as(memory)\n\n            # Decode the next token\n            out = model.decode(memory, src_mask, current_sequence, tgt_mask)\n            prob = model.generator(out[:, -1]) # Get probabilities for the last token\n            log_probs = prob.log_softmax(dim=-1) # Convert to log probabilities\n\n            # Get top 'beam_size' next tokens and their log probabilities\n            top_k_log_probs, top_k_indices = log_probs.topk(beam_size)\n\n            for i in range(beam_size):\n                next_token_log_prob = top_k_log_probs[0, i].item()\n                next_token = top_k_indices[0, i].item()\n\n                # Extend the current sequence with the new token\n                extended_sequence = torch.cat(\n                    [current_sequence, torch.full((1, 1), next_token, dtype=torch.long, device=device)], dim=1\n                )\n                new_candidates.append((log_prob + next_token_log_prob, extended_sequence))\n\n        # Sort all new candidates by their log probability and select the top 'beam_size'\n        candidates = sorted(new_candidates, key=lambda x: x[0], reverse=True)[:beam_size]\n\n        # If all best candidates have ended, we can stop early (optional optimization)\n        if all(cand[1][0, -1].item() == end_symbol for cand in candidates):\n            break\n\n    # Return the best sequence (highest log probability)\n    return candidates[0][1].cpu().squeeze().tolist()\n","metadata":{"id":"n8D7aakj4c89","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.883770Z","iopub.execute_input":"2025-12-16T21:37:06.885023Z","iopub.status.idle":"2025-12-16T21:37:06.912093Z","shell.execute_reply.started":"2025-12-16T21:37:06.884975Z","shell.execute_reply":"2025-12-16T21:37:06.910983Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def rate(step, d_model, factor, warmup):\n    \"\"\"\n    we have to default the step to 1 for LambdaLR function\n    to avoid zero raising to negative power.\n    \"\"\"\n    if step == 0:\n        step = 1\n    return factor * (\n        d_model ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n    )","metadata":{"id":"aFvkdmPtjlsl","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.913173Z","iopub.execute_input":"2025-12-16T21:37:06.913474Z","iopub.status.idle":"2025-12-16T21:37:06.934253Z","shell.execute_reply.started":"2025-12-16T21:37:06.913449Z","shell.execute_reply":"2025-12-16T21:37:06.932924Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"#loss\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, padding_idx = 0, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n        self.padding_idx = padding_idx\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 2))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            true_dist[:, self.padding_idx] = 0\n            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n            if mask.dim() > 0:\n                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","metadata":{"id":"I8wG2ybcn6W0","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.935326Z","iopub.execute_input":"2025-12-16T21:37:06.935621Z","iopub.status.idle":"2025-12-16T21:37:06.951802Z","shell.execute_reply.started":"2025-12-16T21:37:06.935599Z","shell.execute_reply":"2025-12-16T21:37:06.950773Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import math\n\ndef evaluate_loss(model, val_dataloader, criterion, device, pad_idx = PAD_ID):\n    \"\"\"\n    Ch·∫°y model tr√™n t·∫≠p Val ƒë·ªÉ t√≠nh Loss v√† Perplexity\n    \"\"\"\n    model.eval() # Chuy·ªÉn sang ch·∫ø ƒë·ªô ƒë√°nh gi√° (t·∫Øt Dropout)\n    total_loss = 0\n    \n    with torch.no_grad(): # T·∫Øt t√≠nh to√°n gradient ƒë·ªÉ ti·∫øt ki·ªám RAM\n        for batch_data in val_dataloader:\n            src, tgt = batch_data[0].to(device), batch_data[1].to(device)\n            batch = Batch(src, tgt, pad=pad_idx)\n            \n            # Forward pass\n            out = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n            out = model.generator(out)\n            \n            # T√≠nh Loss\n            loss = criterion(out.contiguous().view(-1, out.size(-1)), \n                             batch.tgt_y.contiguous().view(-1))\n            total_loss += loss.item()\n            \n    avg_loss = total_loss / len(val_dataloader)\n    \n    # T√≠nh Perplexity (PPL) = exp(Loss)\n    # PPL c√†ng th·∫•p c√†ng t·ªët\n    try:\n        ppl = math.exp(avg_loss)\n    except OverflowError:\n        ppl = float('inf')\n        \n    model.train() # Quan tr·ªçng: Chuy·ªÉn l·∫°i v·ªÅ ch·∫ø ƒë·ªô train cho epoch sau\n    return avg_loss, ppl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.953154Z","iopub.execute_input":"2025-12-16T21:37:06.953544Z","iopub.status.idle":"2025-12-16T21:37:06.977929Z","shell.execute_reply.started":"2025-12-16T21:37:06.953518Z","shell.execute_reply":"2025-12-16T21:37:06.977016Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import torch\n\ndef translate_sentence(sentence, model, sp_model, device, max_len=100):\n    model.eval() # Chuy·ªÉn sang ch·∫ø ƒë·ªô ƒë√°nh gi√°\n    \n    # 1. Tokenize c√¢u ƒë·∫ßu v√†o\n    # Th√™m BOS v√† EOS gi·ªëng h·ªát l√∫c train\n    tokens = [BOS_ID] + sp_model.encode_as_ids(sentence) + [EOS_ID]\n    src = torch.tensor(tokens).long().unsqueeze(0).to(device) # (1, seq_len)\n    \n    # 2. T·∫°o mask\n    src_mask = (src != PAD_ID).unsqueeze(-2)\n    \n    # 3. Encoder\n    with torch.no_grad():\n        memory = model.encode(src, src_mask)\n        \n    # 4. Beam Search (H√†m n√†y b·∫°n ƒë√£ c√≥ trong code g·ªëc)\n    # L∆∞u √Ω: C·∫ßn ƒë·∫£m b·∫£o h√†m beam_search ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n    output_ids = beam_search(model, memory, src_mask, BOS_ID, EOS_ID, max_len, beam_size=3, device=device)\n    \n    # 5. Decode\n    if EOS_ID in output_ids:\n        output_ids = output_ids[:output_ids.index(EOS_ID)]\n        \n    translation = sp_model.decode(output_ids)\n    \n    model.train() # Chuy·ªÉn l·∫°i v·ªÅ mode train\n    return translation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:06.978992Z","iopub.execute_input":"2025-12-16T21:37:06.979400Z","iopub.status.idle":"2025-12-16T21:37:07.000219Z","shell.execute_reply.started":"2025-12-16T21:37:06.979371Z","shell.execute_reply":"2025-12-16T21:37:06.999136Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Thi·∫øt l·∫≠p thi·∫øt b·ªã\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nD_MODEL = 128\nVOCAB_SIZE = sp.get_piece_size() \nD_FF = 1024\nLAYERS=4\nH=4\n# Quan tr·ªçng: LabelSmoothingLoss c·∫ßn input ƒë√∫ng k√≠ch th∆∞·ªõc vocab\ncriterion = LabelSmoothingLoss(classes=VOCAB_SIZE, padding_idx=PAD_ID, smoothing=0.1)\n\n# Kh·ªüi t·∫°o m√¥ h√¨nh (ƒë·∫£m b·∫£o h√†m make_model ƒë√£ ƒë∆∞·ª£c s·ª≠a nh∆∞ m·ª•c 3)\nmodel = make_model(src_vocab=VOCAB_SIZE, tgt_vocab=VOCAB_SIZE, \n                   layers=LAYERS, h=H, d_model=D_MODEL, d_ff=D_FF, dropout=0.1)\n\nmodel.to(device)\n\nprint(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:07.004770Z","iopub.execute_input":"2025-12-16T21:37:07.005140Z","iopub.status.idle":"2025-12-16T21:37:07.240052Z","shell.execute_reply.started":"2025-12-16T21:37:07.005112Z","shell.execute_reply":"2025-12-16T21:37:07.239032Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nModel created with 9064576 parameters.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"def train_with_validation_and_checkpointing(\n    model, train_dataloader, val_dataloader, criterion, optimizer, lr_scheduler, \n    device, pad_idx, sp_model, num_epochs, sample_sentence,\n    start_epoch=0, best_val_loss=float('inf')\n):\n    model.train()\n    scaler = torch.amp.GradScaler(\"cuda\")\n    print(f\"{'='*10} B·∫ÆT ƒê·∫¶U TRAINING V·ªöI  {'='*10}\")\n\n    # Kh·ªüi t·∫°o history ƒë·ªÉ l∆∞u k·∫øt qu·∫£\n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"val_ppl\": []\n    }\n    print(f\"{'='*10} B·∫ÆT ƒê·∫¶U TRAINING T·ª™ EPOCH {start_epoch + 1} {'='*10}\")\n    for epoch in range(start_epoch, num_epochs):\n        print(f\"\\n--- Epoch {epoch + 1}/{num_epochs} ---\")\n        \n        # 1. TRAINING LOOP\n        model.train()\n        train_loss = 0\n        pbar = tqdm(train_dataloader, desc=\"Training\")\n        \n        for i, batch_data in enumerate(pbar):\n            src, tgt = batch_data[0].to(device), batch_data[1].to(device)\n            # Skip c√¢u qu√° d√†i\n            if src.size(1) > 150 or tgt.size(1) > 150: continue\n            \n            batch = Batch(src, tgt, pad=pad_idx)\n            with torch.amp.autocast(device_type=\"cuda\"):\n            # with autocast():\n                out = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n                out = model.generator(out)\n                loss = criterion(out.contiguous().view(-1, out.size(-1)), batch.tgt_y.contiguous().view(-1))\n\n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            lr_scheduler.step()\n            \n            train_loss += loss.item()\n            pbar.set_postfix({\"loss\": loss.item(), \"lr\": optimizer.param_groups[0][\"lr\"]})\n            \n        avg_train_loss = train_loss / len(train_dataloader)\n        \n        # 2. VALIDATION LOOP (Sau khi xong 1 epoch train)\n        print(\"ƒêang ch·∫°y validate...\")\n        avg_val_loss, val_ppl = evaluate_loss(model, val_dataloader, criterion, device, pad_idx)\n        \n        print(f\"üìå K·∫æT QU·∫¢ EPOCH {epoch+1}:\")\n        print(f\"   - Train Loss: {avg_train_loss:.4f}\")\n        print(f\"   - Val Loss:   {avg_val_loss:.4f} (Perplexity: {val_ppl:.2f})\")\n        \n        # 1. L∆∞u checkpoint c·ªßa epoch cu·ªëi c√πng (ƒë·ªÉ ph√≤ng khi b·ªã crash gi·ªØa ch·ª´ng)\n        torch.save({\n            'epoch': epoch + 1, # L∆∞u epoch k·∫ø ti·∫øp s·∫Ω ch·∫°y\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': lr_scheduler.state_dict(),\n            'best_val_loss': best_val_loss,\n        }, \"last_vlsp_checkpoint.pth\")\n        \n        # 2. L∆∞u checkpoint t·ªët nh·∫•t (d·ª±a tr√™n val_loss)\n        if avg_val_loss < best_val_loss:\n            print(f\"‚úÖ Loss gi·∫£m ({best_val_loss:.4f} -> {avg_val_loss:.4f}). L∆∞u best checkpoint!\")\n            best_val_loss = avg_val_loss\n            # G√≥i t·∫•t c·∫£ v√†o m·ªôt dictionary\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': lr_scheduler.state_dict(),\n                'best_val_loss': best_val_loss,\n            }, \"best_vlsp_checkpoint.pth\")\n        else:\n            print(f\"‚ö†Ô∏è Loss kh√¥ng gi·∫£m (Best: {best_val_loss:.4f}). Kh√¥ng c·∫≠p nh·∫≠t best checkpoint.\")\n            \n        # L∆∞u k·∫øt qu·∫£ v√†o history\n        history[\"train_loss\"].append(avg_train_loss)\n        history[\"val_loss\"].append(avg_val_loss)\n        history[\"val_ppl\"].append(val_ppl)\n        # 4. D·ªãch th·ª≠\n        if sp_model:\n            try:\n                pred = translate_sentence(sample_sentence, model, sp_model, device)\n                print(f\"   - D·ªãch th·ª≠: {pred}\")\n            except: pass\n            \n    print(f\"\\nTraining ho√†n t·∫•t! Best Val Loss: {best_val_loss:.4f}\")\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:07.241466Z","iopub.execute_input":"2025-12-16T21:37:07.241768Z","iopub.status.idle":"2025-12-16T21:37:07.292961Z","shell.execute_reply.started":"2025-12-16T21:37:07.241746Z","shell.execute_reply":"2025-12-16T21:37:07.291906Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def load_checkpoint(model, optimizer, scheduler, filename=\"last_checkpoint.pth\"):\n    start_epoch = 0\n    best_val_loss = float('inf')\n    \n    if os.path.exists(filename):\n        print(f\"--- ƒêang t·∫£i checkpoint t·ª´: {filename} ---\")\n        checkpoint = torch.load(filename, map_location=device)\n        \n        # Kh√¥i ph·ª•c tr·∫°ng th√°i\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        \n        # L·∫•y th√¥ng tin epoch v√† loss ƒë·ªÉ train ti·∫øp\n        start_epoch = checkpoint['epoch']\n        best_val_loss = checkpoint['best_val_loss']\n        \n        print(f\"‚úÖ Kh√¥i ph·ª•c th√†nh c√¥ng! S·∫µn s√†ng train ti·∫øp t·ª´ Epoch {start_epoch + 1}.\")\n        print(f\"   - Best Val Loss ƒë√£ ghi nh·∫≠n: {best_val_loss:.4f}\")\n        print(f\"   - Learning Rate hi·ªán t·∫°i: {optimizer.param_groups[0]['lr']:.6f}\")\n    else:\n        print(\"--- Kh√¥ng t√¨m th·∫•y checkpoint. B·∫Øt ƒë·∫ßu training t·ª´ ƒë·∫ßu. ---\")\n        \n    return start_epoch, best_val_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:07.294087Z","iopub.execute_input":"2025-12-16T21:37:07.294476Z","iopub.status.idle":"2025-12-16T21:37:07.323048Z","shell.execute_reply.started":"2025-12-16T21:37:07.294403Z","shell.execute_reply":"2025-12-16T21:37:07.322056Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_metrics(history):\n    \"\"\"\n    V·∫Ω ƒë·ªì th·ªã Train Loss, Val Loss, v√† Val Perplexity.\n    \"\"\"\n    # L·∫•y d·ªØ li·ªáu t·ª´ history\n    train_loss = history['train_loss']\n    val_loss = history['val_loss']\n    val_ppl = history['val_ppl']\n    epochs = range(1, len(train_loss) + 1)\n    \n    # T·∫°o figure v·ªõi 2 subplot\n    plt.figure(figsize=(14, 5))\n    \n    # --- Bi·ªÉu ƒë·ªì 1: Loss ---\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # --- Bi·ªÉu ƒë·ªì 2: Perplexity ---\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, val_ppl, 'go-', label='Validation Perplexity')\n    plt.title('Validation Perplexity')\n    plt.xlabel('Epochs')\n    plt.ylabel('Perplexity')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:37:07.324114Z","iopub.execute_input":"2025-12-16T21:37:07.324508Z","iopub.status.idle":"2025-12-16T21:37:07.346216Z","shell.execute_reply.started":"2025-12-16T21:37:07.324475Z","shell.execute_reply":"2025-12-16T21:37:07.345152Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# EXE","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\noptimizer = torch.optim.Adam(model.parameters(), lr=1.0, betas=(0.9, 0.98), eps=1e-9)\nlr_scheduler = LambdaLR(optimizer=optimizer,\n    lr_lambda=lambda step: rate(step, d_model=D_MODEL, factor=1, warmup=4000))\n\n# --- 2. LOAD CHECKPOINT (N·∫æU C√ì) ---\n# H√†m n√†y s·∫Ω t·ª± ƒë·ªông ƒëi·ªÅn tr·∫°ng th√°i ƒë√£ l∆∞u v√†o c√°c object ·ªü tr√™n\nstart_epoch, best_val_loss = load_checkpoint(model, optimizer, lr_scheduler,\n                                             '/kaggle/input/best/pytorch/default/1/best_checkpoint.pth')\n# Train l·∫°i t·ª´ ƒë·∫ßu ho·∫∑c train ti·∫øp \n\ntry:\n    history = train_with_validation_and_checkpointing(\n        model=model,\n        train_dataloader=train_loader,\n        val_dataloader=val_loader,\n        criterion=criterion,\n        optimizer=optimizer,         # Truy·ªÅn optimizer ƒë√£ ƒë∆∞·ª£c load state\n        lr_scheduler=lr_scheduler,   # Truy·ªÅn scheduler ƒë√£ ƒë∆∞·ª£c load state\n        device=device,\n        pad_idx=PAD_ID,\n        sp_model=sp,\n        num_epochs=25,               # V√≠ d·ª• mu·ªën train t·ªïng c·ªông 20 epochs\n        sample_sentence=\"Knowledge, practices in public health service utilization among health insurance card‚Äôs holders and influencing factors in Vientiane, Lao\",\n        start_epoch=start_epoch,     # B·∫Øt ƒë·∫ßu t·ª´ epoch ƒë√£ load\n        best_val_loss=best_val_loss  # D√πng best_loss ƒë√£ load ƒë·ªÉ so s√°nh\n    )\nexcept KeyboardInterrupt:\n    print(\"ƒê√£ d·ª´ng training th·ªß c√¥ng.\")\nfinally:\n    # 2. Sau khi ch·∫°y xong (d√π th√†nh c√¥ng hay th·∫•t b·∫°i), h√£y ki·ªÉm tra xem 'history' c√≥ gi√° tr·ªã h·ª£p l·ªá kh√¥ng\n    if history is not None:\n        print(\"Training ho√†n t·∫•t. ƒêang v·∫Ω ƒë·ªì th·ªã...\")\n        plot_metrics(history)\n    else:\n        print(\"Kh√¥ng c√≥ d·ªØ li·ªáu history ƒë·ªÉ v·∫Ω ƒë·ªì th·ªã do training kh√¥ng th√†nh c√¥ng.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:59:02.220417Z","iopub.execute_input":"2025-12-16T21:59:02.220793Z","execution_failed":"2025-12-16T22:51:46.158Z"}},"outputs":[{"name":"stdout","text":"--- ƒêang t·∫£i checkpoint t·ª´: /kaggle/input/best/pytorch/default/1/best_checkpoint.pth ---\n‚úÖ Kh√¥i ph·ª•c th√†nh c√¥ng! S·∫µn s√†ng train ti·∫øp t·ª´ Epoch 21.\n   - Best Val Loss ƒë√£ ghi nh·∫≠n: 1.2044\n   - Learning Rate hi·ªán t·∫°i: 0.000218\n========== B·∫ÆT ƒê·∫¶U TRAINING V·ªöI  ==========\n========== B·∫ÆT ƒê·∫¶U TRAINING T·ª™ EPOCH 21 ==========\n\n--- Epoch 21/25 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 282/282 [04:44<00:00,  1.01s/it, loss=1.85, lr=0.000218]","output_type":"stream"},{"name":"stdout","text":"ƒêang ch·∫°y validate...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"üìå K·∫æT QU·∫¢ EPOCH 21:\n   - Train Loss: 1.9698\n   - Val Loss:   1.8546 (Perplexity: 6.39)\n‚ö†Ô∏è Loss kh√¥ng gi·∫£m (Best: 1.2044). Kh√¥ng c·∫≠p nh·∫≠t best checkpoint.\n   - D·ªãch th·ª≠:  ‚Åá in c·∫£m ∆°n r·∫•t nhi·ªÅu l·∫ßn.\n\n--- Epoch 22/25 ---\n","output_type":"stream"},{"name":"stderr","text":"Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 135/282 [02:04<02:23,  1.02it/s, loss=1.87, lr=0.000218]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# import torch\n# import os\n# import pandas as pd  # D√πng pandas ƒë·ªÉ hi·ªÉn th·ªã b·∫£ng cho ƒë·∫πp\n\n# # 1. C·∫•u h√¨nh Model (Ph·∫£i kh·ªõp 100% v·ªõi l√∫c train)\n# # N·∫øu b·∫°n ƒë√£ ƒë·ªïi config l√∫c train th√¨ s·ª≠a l·∫°i ·ªü ƒë√¢y nh√©\n# real_vocab_size = sp.get_piece_size()\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # 2. Kh·ªüi t·∫°o l·∫°i model r·ªóng\n# model_eval = make_model(src_vocab=real_vocab_size, tgt_vocab=real_vocab_size, \n#                         layers=layers, h=h, d_model=d_model, d_ff=d_ff, dropout=0.1)\n# model_eval.to(device)\n\n# # 3. Danh s√°ch c√¢u c·∫ßn soi (Test Set)\n# sentences_to_track = [\n#     \"Thank you very much for your time.\",  # C√¢u \"huy·ªÅn tho·∫°i\" c·ªßa b·∫°n\n#     \"The world is changing very fast.\",    # C√¢u v·ªÅ s·ª± thay ƒë·ªïi\n#     \"Dickhead.\",                           # C√¢u kinh ƒëi·ªÉn\n#     \"We need to protect our environment.\", # C√¢u ph·ª©c t·∫°p h∆°n ch√∫t\n#     \"Education is important.\"              # C√¢u ƒë∆°n gi·∫£n\n# ]\n\n# # 4. H√†m ch·∫°y v√≤ng l·∫∑p qua c√°c Epoch\n# def compare_epochs(start_ep=1, end_ep=10):\n#     results = {sent: [] for sent in sentences_to_track}\n#     epochs_found = []\n\n#     print(f\"ƒêang so s√°nh t·ª´ Epoch {start_ep} ƒë·∫øn {end_ep}...\\n\")\n\n#     for epoch in range(start_ep, end_ep + 1):\n#         filename = f\"transformer_epoch_{epoch}.pth\"\n        \n#         if not os.path.exists(filename):\n#             print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file {filename}, b·ªè qua.\")\n#             continue\n            \n#         # Load weights v√†o model\n#         model_eval.load_state_dict(torch.load(filename, map_location=device))\n#         epochs_found.append(f\"Ep {epoch}\")\n        \n#         # D·ªãch t·ª´ng c√¢u\n#         for sent in sentences_to_track:\n#             try:\n#                 # D√πng h√†m translate_sentence b·∫°n ƒë√£ c√≥\n#                 pred = translate_sentence(sent, model_eval, sp, device)\n#                 results[sent].append(pred)\n#             except:\n#                 results[sent].append(\"Error\")\n\n#     return results, epochs_found\n\n# # --- TH·ª∞C THI V√Ä HI·ªÇN TH·ªä ---\n# comparison_data, cols = compare_epochs(1, 10) # Ch·ªânh s·ªë epoch t√πy theo file b·∫°n c√≥\n\n# # Hi·ªÉn th·ªã k·∫øt qu·∫£ (Gom nh√≥m theo t·ª´ng c√¢u ƒë·ªÉ d·ªÖ so s√°nh s·ª± ti·∫øn h√≥a)\n# for sent, translations in comparison_data.items():\n#     print(f\"üî¥ SRC: {sent}\")\n#     print(\"-\" * 60)\n#     # T·∫°o DataFrame nh·ªè ƒë·ªÉ hi·ªÉn th·ªã cho ƒë·∫πp\n#     df = pd.DataFrame(translations, index=cols, columns=[\"B·∫£n d·ªãch\"])\n#     print(df)\n#     print(\"\\n\" + \"=\"*60 + \"\\n\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-12-16T21:41:13.206051Z","iopub.status.idle":"2025-12-16T21:41:13.206442Z","shell.execute_reply.started":"2025-12-16T21:41:13.206251Z","shell.execute_reply":"2025-12-16T21:41:13.206269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Test th·ª≠ xem Tokenizer x·ª≠ l√Ω ch·ªØ Xin ra sao\n# print(\"Tokenize 'Xin':\", sp.encode_as_pieces(\"Xin\"))\n# print(\"Tokenize 'xin':\", sp.encode_as_pieces(\"xin\"))\n# print(\"Tokenize 'Xin c·∫£m ∆°n':\", sp.encode_as_pieces(\"Xin c·∫£m ∆°n\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:41:13.207743Z","iopub.status.idle":"2025-12-16T21:41:13.208127Z","shell.execute_reply.started":"2025-12-16T21:41:13.207938Z","shell.execute_reply":"2025-12-16T21:41:13.207956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate_batch_greedy(src_batch, src_mask, model, max_len=100):\n    \"\"\"\n    D·ªãch m·ªôt batch c√¢u b·∫±ng ph∆∞∆°ng ph√°p Greedy Search.\n    R·∫•t nhanh nh∆∞ng ch·∫•t l∆∞·ª£ng c√≥ th·ªÉ k√©m h∆°n Beam Search m·ªôt ch√∫t.\n    \"\"\"\n    batch_size = src_batch.size(0)\n    device = src_batch.device\n    \n    # 1. Ch·∫°y Encoder M·ªòT L·∫¶N cho c·∫£ batch\n    with torch.no_grad():\n        memory = model.encode(src_batch, src_mask)\n\n    # 2. Kh·ªüi t·∫°o decoder input v·ªõi token BOS cho t·∫•t c·∫£ c√°c c√¢u trong batch\n    ys = torch.full((batch_size, 1), BOS_ID, dtype=torch.long, device=device)\n\n    # 3. V√≤ng l·∫∑p gi·∫£i m√£ Greedy\n    for _ in range(max_len - 1):\n        with torch.no_grad():\n            # Ch·∫°y decoder\n            tgt_mask = subsequent_mask(ys.size(1)).type_as(src_mask.data)\n            out = model.decode(memory, src_mask, ys, tgt_mask)\n            \n            # L·∫•y token cu·ªëi c√πng v√† ch·∫°y qua generator\n            prob = model.generator(out[:, -1])\n            \n            # L·∫•y token c√≥ x√°c su·∫•t cao nh·∫•t (Greedy)\n            _, next_word = torch.max(prob, dim=1)\n            \n            # N·ªëi token m·ªõi v√†o chu·ªói k·∫øt qu·∫£\n            ys = torch.cat([ys, next_word.unsqueeze(1)], dim=1)\n            \n    return ys\n\ndef calculate_bleu_fast(model, dataloader, sp_model, device):\n    \"\"\"T√≠nh BLEU score b·∫±ng ph∆∞∆°ng ph√°p d·ªãch theo l√¥.\"\"\"\n    print(\"B·∫Øt ƒë·∫ßu t√≠nh BLEU score (phi√™n b·∫£n nhanh)...\")\n    \n    bleu = BLEUScore(n_gram=4)\n    model.eval()\n    \n    all_preds = []\n    all_refs = []\n\n    for batch in tqdm(dataloader, desc=\"D·ªãch theo l√¥\"):\n        src, tgt = batch[0].to(device), batch[1].to(device)\n        \n        # T·∫°o source mask\n        src_mask = (src != PAD_ID).unsqueeze(-2)\n        \n        # D·ªãch c·∫£ batch b·∫±ng Greedy Search\n        pred_ids_batch = translate_batch_greedy(src, src_mask, model)\n        \n        # Decode t·ª´ ID sang text v√† l∆∞u k·∫øt qu·∫£\n        for i in range(pred_ids_batch.size(0)):\n            # L·∫•y c√¢u g·ªëc (reference)\n            ref_ids = tgt[i, 1:] # B·ªè BOS\n            ref_text = sp_model.decode([id for id in ref_ids.tolist() if id != PAD_ID and id != EOS_ID])\n            all_refs.append([ref_text])\n            \n            # L·∫•y c√¢u d·ªãch (prediction)\n            pred_ids = pred_ids_batch[i, 1:] # B·ªè BOS\n            pred_text = sp_model.decode([id for id in pred_ids.tolist() if id != PAD_ID and id != EOS_ID])\n            all_preds.append(pred_text)\n            \n    # T√≠nh ƒëi·ªÉm BLEU cu·ªëi c√πng\n    score = bleu(all_preds, all_refs)\n    \n    print(\"\\n\" + \"=\"*30)\n    print(f\"K·∫æT QU·∫¢ BLEU SCORE (Nhanh): {score.item():.4f}\")\n    print(\"=\"*30)\n    \n    return score.item(), all_preds, all_refs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:45:31.938389Z","iopub.execute_input":"2025-12-16T21:45:31.938763Z","iopub.status.idle":"2025-12-16T21:45:31.955201Z","shell.execute_reply.started":"2025-12-16T21:45:31.938734Z","shell.execute_reply":"2025-12-16T21:45:31.953900Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# 1. ƒê∆∞·ªùng d·∫´n file Test (ki·ªÉm tra l·∫°i trong folder input c·ªßa b·∫°n xem t√™n ch√≠nh x√°c l√† g√¨)\n# Th∆∞·ªùng l√† tst2012 ho·∫∑c tst2013\ntest_src_file = \"/kaggle/input/medicaldataset-vlsp/MedicalDataset_VLSP/public_test.en.txt\"\ntest_tgt_file = \"/kaggle/input/medicaldataset-vlsp/MedicalDataset_VLSP/public_test.vi.txt\"\n\n# 2. T·∫°o Dataset cho t·∫≠p Test\nif os.path.exists(test_src_file):\n    print(\"ƒêang t·∫°o Test Dataset...\")\n    test_loader_for_bleu = DataLoader(\n        Subset(IWSLTDataset(test_src_file, test_tgt_file), range(64)), \n        batch_size= 8,\n        shuffle=False, # Kh√¥ng shuffle khi test\n        collate_fn=collate_batch\n    )\n    # 3. T√≠nh BLEU tr√™n t·∫≠p Test \n    print(\"ƒêang ch·∫•m ƒëi·ªÉm tr√™n t·∫≠p Test (D·ªØ li·ªáu l·∫°)...\")\n    score, preds, refs = calculate_bleu_fast(\n        model=model,\n        dataloader=test_loader_for_bleu,\n        sp_model=sp,\n        device=device\n    )\nelse:\n    print(\"Kh√¥ng t√¨m th·∫•y file Test. B·∫°n h√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n trong Input.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:46:46.705748Z","iopub.execute_input":"2025-12-16T21:46:46.706080Z","iopub.status.idle":"2025-12-16T21:47:13.861153Z","shell.execute_reply.started":"2025-12-16T21:46:46.706055Z","shell.execute_reply":"2025-12-16T21:47:13.860290Z"}},"outputs":[{"name":"stdout","text":"ƒêang t·∫°o Test Dataset...\nƒêang ƒë·ªçc v√† l·ªçc d·ªØ li·ªáu...\nHo√†n t·∫•t load data. S·ªë c·∫∑p c√¢u h·ª£p l·ªá: 3000\nƒêang ch·∫•m ƒëi·ªÉm tr√™n t·∫≠p Test (D·ªØ li·ªáu l·∫°)...\nB·∫Øt ƒë·∫ßu t√≠nh BLEU score (phi√™n b·∫£n nhanh)...\n","output_type":"stream"},{"name":"stderr","text":"D·ªãch theo l√¥: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:26<00:00,  3.36s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n==============================\nK·∫æT QU·∫¢ BLEU SCORE (Nhanh): 0.1111\n==============================\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"score, preds, refs = calculate_bleu_fast(\n    model=model,\n    dataloader=test_loader_for_bleu,\n    sp_model=sp,\n    device=device\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-16T22:51:46.159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# # N√©n to√†n b·ªô file trong working th√†nh output.zip\n# os.system(\"zip -r output.zip .\")\n\n# from IPython.display import FileLink\n# print(\"B·∫•m v√†o ƒë√¢y ƒë·ªÉ t·∫£i tr·ªçn b·ªô (ZIP):\")\n# FileLink(r'output.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:41:13.212765Z","iopub.status.idle":"2025-12-16T21:41:13.213194Z","shell.execute_reply.started":"2025-12-16T21:41:13.213058Z","shell.execute_reply":"2025-12-16T21:41:13.213072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# os.remove(\"best_model_iwslt.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T21:41:13.214743Z","iopub.status.idle":"2025-12-16T21:41:13.215034Z","shell.execute_reply.started":"2025-12-16T21:41:13.214904Z","shell.execute_reply":"2025-12-16T21:41:13.214917Z"}},"outputs":[],"execution_count":null}]}